{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code was developed for the \"SOCIAL IMPACT HACKATHON Analyzing our Environmental Tweet-Bank\".\n",
    "## Final results: We won the \"Popular Vote\" as well as the \"Silver Badge\"\n",
    "Tweet Queries:\n",
    "We decided to do our own queries instead of using the already available datasets due to the following reasons:\n",
    "1-Looking at the questions we were trying to asnwer there have been problems as:\n",
    "    a.The provided dataset did not have certain columns i.e. for user to analyze \"influencers\"\n",
    "    b.Many key words in the search were missing such as donation etc.\n",
    "2-Many tweets were irrelevant i.e. heavy political focus in US or other part of the world not related to the problem.\n",
    "\n",
    "webapp: https://github.com/aelnahas/greentweets\n",
    "https://greentweets.netlify.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a sloppy way to use twitter credentials (you need to complete usecase requirement for your twitter developer account)\n",
    "api_key=\"*****\"\n",
    "api_key_secret=\"*****\"\n",
    "bearer_token=\"*****\"\n",
    "consumer_key=api_key\n",
    "consumer_key_secret=api_key_secret\n",
    "access_token=\"*****\"\n",
    "access_token_secret=\"\"*****\"\"\n",
    "app_id=\"*****\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing required libraries - tweepy to get the tweets of course\n",
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on your own account\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test2 for the hackathon usecase\n",
    "cursor=tweepy.Cursor(api.search,q='\"Lake Ontario\" clean', tweet_mode=\"extended\").items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shaping our desired dataset with related and permissable fields\n",
    "number_of_tw=200\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='\"Lake Ontario\" clean', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'Clean','inclakeontario':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeating the process for other keywords\n",
    "number_of_tw=200\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='\"Lake Ontario\" garbage', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'garbage','inclakeontario':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeating the process for other keywords\n",
    "number_of_tw=200\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='\"Lake Ontario\" environment', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'environment','inclakeontario':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeating the process for other keywords\n",
    "number_of_tw=200\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='volunteer environment', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'volunteer+litter+Ontario','inclakeontario':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeating the process for other keywords\n",
    "number_of_tw=200\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='lake cleanup', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'lake cleanup','inclakeontario':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_tw=100\n",
    "tw_id,tweets,likes,retweets,time,followers,friends,location=[],[],[],[],[],[],[],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tweepy.Cursor(api.search,q='Donation ontario environment', tweet_mode=\"extended\").items(number_of_tw):\n",
    "    tw_id.append(i.id)\n",
    "    tweets.append(i.full_text)\n",
    "    likes.append(i.favorite_count)\n",
    "    retweets.append(i.retweet_count)\n",
    "    time.append(i.created_at)\n",
    "    followers.append(i.user.followers_count)\n",
    "    friends.append(i.user.friends_count)\n",
    "    location.append(i.user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5=pd.DataFrame({'id':tw_id,'tweets':tweets,'likes':likes,'retweets':retweets,'time':time,'followers':followers,\n",
    "                 'friends':friends, 'location':location, 'key_word':'donation','inclakeontario':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Defnitely looping is mroe efficient but we were exploring the keywords at each time and coming up with a new keyword idea adding them to the program. So this is not an optimal way by any means. you can continue with more keywords and add them all together as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes=[df,df1,df2,df3,df4,df5]\n",
    "df_final=pd.concat(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(r'C:\\...\\tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aletrnatively you could drop the duplicates before finalizing your dataset.\n",
    "df_nodup=df_final.drop_duplicates(subset=['id','tweets','likes','retweets','time','followers','friends', 'location'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.to_csv(r'C:\\...\\tweets_nodup.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
